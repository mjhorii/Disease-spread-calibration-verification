{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Use kernel \"ABM_env\" -- see README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import cProfile\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from amcmc import ammcmc\n",
    "\n",
    "from ABM import SEIR_multiple_pops\n",
    "from run_simulations import simulate_epidemic_1d\n",
    "from CalibrationMethod1_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "m = 1 #number of populations\n",
    "centers = np.array([[0,0]]) #theta, phi or x, y\n",
    "spread = np.array([0.1]) #standard deviation of normal distribution\n",
    "pop = np.array([100]) #population\n",
    "A_1 = 0.01 #theta or x mobility (azimuth mobility)\n",
    "A_2 = 0.01 #phi or y mobility (inclination mobility)\n",
    "R = 1 #radius\n",
    "d_IU = 0.005\n",
    "E_0 = np.array([0]) #fraction of initially exposed\n",
    "I_0 = np.array([0.01]) #fraction of initially infected\n",
    "S_0 = np.array([0.99]) #fraction of initially susceptible\n",
    "T_E = 11.6 #time from exposure to infectious\n",
    "T_E_stdev = 1.9 #standard deviation of exposure time\n",
    "T_I = 18.49 #incubation time\n",
    "T_I_stdev = 3.71 #standard deviation of infection time\n",
    "del_t = 0.1 #time step\n",
    "verlet_iter = 300 #number of steps between updating verlet list\n",
    "T = 300\n",
    "rand_seed = 1\n",
    "g = None\n",
    "al = None\n",
    "jumping_times = np.zeros(int(T/del_t)+1)\n",
    "jump_prob = 0.5\n",
    "spherical = None\n",
    "dist = 'Gamma'\n",
    "\n",
    "time_vec = np.linspace(0,T,int(T/del_t)+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training data\n",
    "data_file = open('./Data/Training Data/One-parameter case/Calibration method 1/new_I_data_One-Pop-Disc.pickle', \"rb\")\n",
    "data = pickle.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "#Load in training data mobility and jumping probability values:\n",
    "parameter_matrix = pd.read_csv('./Data/Training Data/One-parameter case/Calibration method 1/variable_parameter_values_One-Pop-Disc.csv', index_col=0).to_numpy()\n",
    "mobilities = parameter_matrix[:,0]\n",
    "jumping_probs = parameter_matrix[:,1]\n",
    "random_seeds = parameter_matrix[:,2]\n",
    "\n",
    "#Reshape to sort by mobility and jumping prob:\n",
    "#based on assumption that data is given in shape (num_of_sims, num_of_time_steps, num_of_subpops)\n",
    "#and that within the list of simulations, the mobility is the \"outer\" index (changes more slowly)\n",
    "#and the jumping probability is the \"inner\" index (changes more quickly)\n",
    "\n",
    "unique_mobilities = np.unique(mobilities) \n",
    "unique_jumping_probs = np.unique(jumping_probs)\n",
    "\n",
    "num_of_mobilities = unique_mobilities.shape[0]\n",
    "num_of_jumping_probs = unique_jumping_probs.shape[0]\n",
    "\n",
    "num_of_random_seeds_per_param_set = round(data.shape[0]/(num_of_mobilities*num_of_jumping_probs)) #assuming equal number of random seeds run for each param set\n",
    "num_of_time_steps = data.shape[1]\n",
    "# num_of_subpops = 2 #assumes 2 sub-populations\n",
    "\n",
    "unique_mobilities = np.unique(mobilities) \n",
    "num_of_mobilities = unique_mobilities.shape[0]\n",
    "\n",
    "data_sorted = np.zeros((num_of_mobilities, num_of_random_seeds_per_param_set, num_of_time_steps))\n",
    "\n",
    "data_raveled = np.ravel(data, order = 'C')\n",
    "data = np.reshape(data_raveled, data_sorted.shape)\n",
    "data = np.swapaxes(data, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing: Sum training dataset along time segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_intervals = 5 #number of time intervals to split up for likelihood calculation\n",
    "\n",
    "time_steps = data.shape[0]\n",
    "intervals = np.arange(0,time_steps,int(time_steps/num_of_intervals))\n",
    "intervals[-1] = intervals[-1]+time_steps%num_of_intervals\n",
    "new_I_per_interval = np.zeros((num_of_intervals,data.shape[1],data.shape[2]))\n",
    "\n",
    "for i in range(num_of_intervals):\n",
    "    data_ = data[intervals[i]:intervals[i+1],:,:]\n",
    "    new_I_per_interval[i,:,:] = np.sum(data_, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking that data makes sense:\n",
    "fig, ax = plt.subplots(constrained_layout=True, dpi = 200)\n",
    "ax.plot(time_vec,data[:,10,15], zorder = 1, label = 'New infections at each time step')\n",
    "ax.set_ylim(0)\n",
    "ax.set_ylabel('Number of new infections at time step')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.stairs(new_I_per_interval[:,10,15], np.array(intervals)/10, fill = True, color = 'grey', alpha = 0.5, zorder = -1, label = 'Sum of infections during time segment')\n",
    "ax2.set_ylabel('Sum of infections during time segment')\n",
    "fig.legend(bbox_to_anchor=(0.9, 0.95))\n",
    "ax.set_xlabel('Time (days)')\n",
    "ax.set_title('Example of initial and segmented dataset for a single run')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate expanded kernel density estimate PDFs for each time segment and mobility value\n",
    "\n",
    "To correctly interpolate between probability distributions, may require KDE values from outside of desired final probability distribution range (0-100 agents). This means we need to calculate KDE values on an extended grid to set up the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------PARAMETER DEFINITIONS----------------------\n",
    "\n",
    "shape_param = 0.5\n",
    "type_ = \"linear\"\n",
    "\n",
    "#--------------------------KDE & MEANS, VARIANCES, STDEVS------------------------------\n",
    "\n",
    "#generate kernel density estimate PDFs for each time segment and mobility value\n",
    "means = np.zeros((num_of_intervals, num_of_mobilities))\n",
    "variances = np.zeros((num_of_intervals, num_of_mobilities))\n",
    "stdevs = np.zeros((num_of_intervals, num_of_mobilities))\n",
    "stdevs_approx = np.zeros((num_of_intervals, num_of_mobilities)) #approximate stdev for 0-stdev pdfs based on replacement approximation function\n",
    "\n",
    "KDE_fns = np.zeros((num_of_intervals, num_of_mobilities))\n",
    "KDE_fns = KDE_fns.astype('object')\n",
    "\n",
    "for interval in tqdm(range(num_of_intervals)):\n",
    "    for mob_ind in range(num_of_mobilities):\n",
    "        means[interval, mob_ind] = np.mean(new_I_per_interval[interval, :, mob_ind])\n",
    "        variances[interval, mob_ind] = np.var(new_I_per_interval[interval, :, mob_ind])\n",
    "        stdevs[interval, mob_ind] = np.std(new_I_per_interval[interval, :, mob_ind])\n",
    "        stdevs_approx[interval, mob_ind] = np.std(new_I_per_interval[interval, :, mob_ind])\n",
    "        try: \n",
    "            kde = gaussian_kde(new_I_per_interval[interval, :, mob_ind]) #generate KDE -- will fail if all values are 0's\n",
    "            KDE_fns[interval, mob_ind] = kde.pdf\n",
    "        except: \n",
    "            mean = means[interval, mob_ind]\n",
    "            KDE_fns[interval, mob_ind], stdev_of_approx_fn = return_zero_stdev_pdf(mean, shape_param, type_)  \n",
    "            stdevs_approx[interval, mob_ind] = stdev_of_approx_fn\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_pdf(sample_point, new_I_per_interval_ref):\n",
    "    sample_point = sample_point[0]\n",
    "    if sample_point<=0.025 and sample_point>=0.005: #don't like to have this hard-coded here either but also interpolation doesn't work otherwise, and computation time is wasted because prior will be 0 anyway\n",
    "        x_grid, KDE, interpolated_mean, interpolated_var, interpolated_stdev_approx = interp_KDE_1d(sample_point, unique_mobilities, \n",
    "              means, variances, stdevs, stdevs_approx,KDE_fns, \n",
    "              mesh_number = 201, mesh_min = 0, mesh_max = 100, \n",
    "                  mesh_min_extended = 0, mesh_max_extended = 100, add_to_stdev = 0, shape_param = 0.5, type_ = \"linear\", mean = mean, renormalize=True)\n",
    "\n",
    "        log_probs = np.zeros_like(KDE)\n",
    "        for interval in range(num_of_intervals):\n",
    "            log_probs[interval] = np.log(KDE[interval](new_I_per_interval_ref[interval]))\n",
    "        if np.sum(log_probs) == -np.inf:\n",
    "            return [-1e100,0]\n",
    "        else:\n",
    "            return [np.sum(log_probs),0]\n",
    "    else:\n",
    "        return [-1e100,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test/sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('./Data/Test Data/One-parameter case/new_I_data_One-Pop-NEW-COMBINED-TEST.pickle', \"rb\") # Combined batch of MCMC\n",
    "data = pickle.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "parameter_matrix = pd.read_csv('./Data/Test Data/One-parameter case/variable_parameter_values_One-Pop-NEW-COMBINED-TEST.csv', index_col=0).to_numpy() \n",
    "\n",
    "#Load in mobility and jumping probability values:\n",
    "mobilities = parameter_matrix[:,0]\n",
    "random_seeds = parameter_matrix[:,1]\n",
    "num_of_samples = mobilities.shape[0]\n",
    "\n",
    "intervals = np.arange(0,num_of_time_steps,int(num_of_time_steps/num_of_intervals))\n",
    "intervals[-1] = intervals[-1]+num_of_time_steps%num_of_intervals\n",
    "new_I_per_interval = np.zeros((num_of_samples, num_of_intervals))\n",
    "\n",
    "for i in range(num_of_intervals):\n",
    "    data_ = data[:,intervals[i]:intervals[i+1]]\n",
    "    print(data_.shape)\n",
    "    new_I_per_interval[:,i] = np.sum(data_, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#------------------Choose Trial dataset (to try to calibrate to):---------------------\n",
    "sample_data_set_ind = 1\n",
    "new_I_per_interval_ref = new_I_per_interval[sample_data_set_ind]\n",
    "print('actual mobility:', mobilities[sample_data_set_ind])\n",
    "RUN_NAME = \"One-Pop\"\n",
    "mob_value = mobilities[sample_data_set_ind]\n",
    "\n",
    "#-----------------Run Trial MCMC---------------------\n",
    "\n",
    "nsamples = 500\n",
    "end = nsamples\n",
    "nburn = 50\n",
    "seed=100\n",
    "\n",
    "# nsamples = 75000\n",
    "# seed=100\n",
    "# nburn = 5000\n",
    "# end = nsamples\n",
    "\n",
    "nskip = 0\n",
    "nthin = 1\n",
    "tmpchn_dir = \"./MCMC_example_results/\"+RUN_NAME\n",
    "logfile_dir = \"./MCMC_example_results/\"+RUN_NAME\n",
    "os.makedirs(tmpchn_dir, exist_ok = True)\n",
    "os.makedirs(logfile_dir, exist_ok = True)\n",
    "\n",
    "tmpchn = tmpchn_dir + \"/amcmc_TMP_ABM_sample_ind_\"+str(sample_data_set_ind)+\".dat\"\n",
    "logfile = logfile_dir + \"/amcmc_LOG_ABM_sample_ind_\"+str(sample_data_set_ind)+\".dat\"\n",
    "\n",
    "if os.path.isfile(tmpchn): #remove previous run file if it exists\n",
    "    os.remove(tmpchn)\n",
    "if os.path.isfile(logfile): #remove previous run file if it exists\n",
    "    os.remove(logfile)\n",
    "\n",
    "opts = {\"nsteps\": nsamples, \"nfinal\": 10000000,\"gamma\": 1,\n",
    "        \"inicov\": np.array([0.001]),\"inistate\": np.array([0.0151]),\n",
    "        \"spllo\": np.array([0.005]),\"splhi\": np.array([0.025]),\n",
    "        \"logfile\": logfile,\"burnsc\":5,\n",
    "        \"nburn\":nburn,\"nadapt\":100,\"coveps\":1.e-10,\"ofreq\":50,\"tmpchn\":tmpchn,'rnseed':sample_data_set_ind\n",
    "        }\n",
    "\n",
    "ndim = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "print('Sampling f_pdf function with AMCMC ...')\n",
    "start_time = time.time()\n",
    "sol=ammcmc(opts,f_pdf,new_I_per_interval_ref)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed AMCMC Run Time: {elapsed_time} seconds\")\n",
    "\n",
    "samples = sol['chain']\n",
    "logprob = sol['minfo'][:,1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "samples = samples[nskip::nthin]\n",
    "logprob = logprob[nskip::nthin]\n",
    "\n",
    "#save data:\n",
    "with open(tmpchn_dir+'/AMCMC_sample_ind_'+str(int(sample_data_set_ind))+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(sol, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Acceptance rate',sol['accr'])\n",
    "print('Mean:',np.mean(samples, axis=0))\n",
    "print('Var:',np.var(samples, axis=0))\n",
    "print('Cov:',np.cov(samples.T)) \n",
    "\n",
    "upper_percentile = np.percentile(sol['chain'][nburn:], 97.5)\n",
    "lower_percentile = np.percentile(sol['chain'][nburn:], 2.5)\n",
    "upper_percentile_50 = np.percentile(sol['chain'][nburn:], 75)\n",
    "lower_percentile_50 = np.percentile(sol['chain'][nburn:], 25)\n",
    "\n",
    "print('95% credible interval upper bound:', upper_percentile) \n",
    "print('95% credible interval lower bound:',lower_percentile)\n",
    "\n",
    "#----------Brute force posterior sampling----------\n",
    "\n",
    "save_dir = './Brute_force_posterior_estimation_example/'+RUN_NAME\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "save_file = save_dir+'/new_I_data_'+str(int(sample_data_set_ind))+'LOG_PROBS.txt'\n",
    "\n",
    "if os.path.isfile(save_file): #remove previous run if it exists\n",
    "    os.remove(save_file)\n",
    "\n",
    "\n",
    "n_m = 15\n",
    "# n_m = 200\n",
    "test_mobilities = np.linspace(unique_mobilities[0]+1E-10, unique_mobilities[-1]-1E-10, n_m)\n",
    "\n",
    "log_probs = np.zeros((test_mobilities.shape[0]))\n",
    "start_time = time.time()\n",
    "for j in range(test_mobilities.shape[0]):\n",
    "    log_probs[j] = f_pdf([test_mobilities[j]], new_I_per_interval_ref)[0]\n",
    "\n",
    "    fout = open(save_file, 'ab')\n",
    "    dataout = np.array([[test_mobilities[j],log_probs[j]]])\n",
    "    np.savetxt(fout, dataout, fmt='%.8e',delimiter=' ', newline='\\n')\n",
    "    fout.close()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed brute force posterior estimation run time: {elapsed_time} seconds\")\n",
    "\n",
    "with open(save_dir+'/new_I_data_'+str(int(sample_data_set_ind))+'LOG_PROBS.pickle', 'wb') as handle:\n",
    "    pickle.dump(log_probs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Plotting-------------------\n",
    "\n",
    "#Plot chain values:\n",
    "nburn = 50\n",
    "fig = plt.figure(dpi = 300)\n",
    "plt.plot(sol['chain'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mobility')\n",
    "plt.title('MCMC chain')\n",
    "plt.show()\n",
    "\n",
    "#Plot histogram, 95% credible interval bounds, true mobility value\n",
    "bins = np.linspace(0.005,0.025,20)\n",
    "counts, bin_edges = np.histogram(sol['chain'][nburn:end], bins = bins)\n",
    "\n",
    "fig, ax1 = plt.subplots(dpi=300, figsize = (4,3))\n",
    "\n",
    "ax1.hist(sol['chain'][nburn:end], bins = bins, label = 'MCMC results')\n",
    "\n",
    "ax1.set_ylim(0,max(counts))\n",
    "ax1.set_ylabel('Frequency in MCMC chain')\n",
    "ax1.set_xlabel('Mobility')\n",
    "\n",
    "plt.axvline(upper_percentile, color = 'red', label = '95% CI')\n",
    "plt.axvline(lower_percentile, color = 'red')\n",
    "plt.axvline(upper_percentile_50, color = 'gold', label = '50% CI')\n",
    "plt.axvline(lower_percentile_50, color = 'gold')\n",
    "\n",
    "plt.axvline(mob_value, color = 'limegreen', label = 'True mobility', linestyle = '--')\n",
    "\n",
    "fig.legend(bbox_to_anchor = (1.3,0.85), borderaxespad=0.)\n",
    "plt.xticks(unique_mobilities[::4])\n",
    "\n",
    "# plt.savefig('figs_final/1d_MCMC_results/histogram-plot-sample'+str(sample_id)+'.png',bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Plot brute force posterior sampling results\n",
    "fig = plt.figure(dpi = 300)\n",
    "plt.plot(test_mobilities, np.exp(log_probs))\n",
    "plt.xlabel('Mobility')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Brute force sampled posterior')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "631b5a63fa3f4765e1351030ef8cf691cfd4d1c32a3a03a6bedf1fe176832cce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
