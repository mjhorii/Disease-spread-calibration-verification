{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "\n",
    "This file processes the results from running batch1d_posterior_estimation.py. The results are included in the data repository -- see README.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Use kernel \"ABM_env\" -- see README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import cProfile\n",
    "import pickle\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import stats\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from amcmc import ammcmc\n",
    "\n",
    "from ABM import SEIR_multiple_pops\n",
    "from run_simulations import simulate_epidemic_1d\n",
    "from CalibrationMethod1_methods import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results from batch MCMC\n",
    "\n",
    "After running code batch1d_posterior_estimation.py, results in folder 1D_calibration_results_071123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load results\n",
    "MCMC_results_folder = \"./Data/MCMC_results/One-Pop\"\n",
    "brute_force_posterior_folder = \"./Data/Brute_force_posterior_estimation/One-Pop\"\n",
    "brute_force_file_prefix = \"/IND\"\n",
    "\n",
    "#Load in mobility values:\n",
    "parameter_matrix = pd.read_csv('./Data/Test Data/One-parameter case/variable_parameter_values_One-Pop-NEW-COMBINED-TEST.csv', index_col=0).to_numpy() # Combined batch of MCMC\n",
    "mobilities = parameter_matrix[:,0]\n",
    "unique_mobilities = np.linspace(0.005,0.025,17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "Set \"sample_id = 513\" to reproduce Fig 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 513\n",
    "nburn = 25000\n",
    "end = 75000\n",
    "\n",
    "MCMC_file = MCMC_results_folder+\"/AMCMC_sample_ind_\"+str(sample_id)+\".pickle\"\n",
    "data_file = open(MCMC_file, \"rb\")\n",
    "sol = pickle.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "mob_value = mobilities[sample_id]\n",
    "upper_percentile = np.percentile(sol['chain'][nburn:end], 97.5)\n",
    "lower_percentile = np.percentile(sol['chain'][nburn:end], 2.5)\n",
    "upper_percentile_50 = np.percentile(sol['chain'][nburn:end], 75)\n",
    "lower_percentile_50 = np.percentile(sol['chain'][nburn:end], 25)\n",
    "\n",
    "\n",
    "# ------------------ Histogram plot --------------------\n",
    "bins = np.linspace(0.005,0.025,20)\n",
    "counts, bin_edges = np.histogram(sol['chain'][nburn:end], bins = bins)\n",
    "\n",
    "fig, ax1 = plt.subplots(dpi=300, figsize = (4,3))\n",
    "\n",
    "ax1.hist(sol['chain'][nburn:end], bins = bins, label = 'MCMC results')\n",
    "\n",
    "ax1.set_ylim(0,max(counts))\n",
    "ax1.set_ylabel('Frequency in MCMC chain')\n",
    "\n",
    "ax1.set_xlabel('Mobility')\n",
    "\n",
    "plt.axvline(upper_percentile, color = 'red', label = '95% CI')\n",
    "plt.axvline(lower_percentile, color = 'red')\n",
    "plt.axvline(upper_percentile_50, color = 'gold', label = '50% CI')\n",
    "plt.axvline(lower_percentile_50, color = 'gold')\n",
    "\n",
    "plt.axvline(mob_value, color = 'limegreen', label = 'True mobility', linestyle = '--')\n",
    "\n",
    "fig.legend(bbox_to_anchor = (1.39,0.7), borderaxespad=0.)\n",
    "plt.xticks(unique_mobilities[::4])\n",
    "\n",
    "# plt.savefig('figs_final/1d_MCMC_results/histogram-plot-sample'+str(sample_id)+'.png',bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#--------- Plot chain/trace plot -------------\n",
    "\n",
    "plt.figure(dpi = 300, figsize = (4,3))\n",
    "plt.plot(np.arange(0, end), sol['chain'][0:end], linewidth=0.6)\n",
    "plt.xlim(0, end)\n",
    "plt.ylabel('Mobility')\n",
    "plt.xlabel('Iterations')\n",
    "# plt.savefig('figs_final/1d_MCMC_results/chain-plot-sample'+str(sample_id)+'.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#------ Plot brute force posterior estimation -------\n",
    "\n",
    "\n",
    "brute_force_file = brute_force_posterior_folder+brute_force_file_prefix+str(sample_id)+\"LOG_PROBS.txt\"\n",
    "brute_likelihood = pd.read_csv(brute_force_file, header=None, sep=\" \").to_numpy()\n",
    "\n",
    "\n",
    "brute_likelihood_y = np.exp(brute_likelihood[:,1])\n",
    "brute_likelihood_x = brute_likelihood[:,0]\n",
    "\n",
    "#normalize to get posterior in terms of probability density using np.trapz:\n",
    "integral = np.trapz(brute_likelihood_y, brute_likelihood_x)\n",
    "brute_posterior = brute_likelihood_y/integral\n",
    "\n",
    "plt.figure(dpi = 300, figsize = (4,3))\n",
    "plt.plot(brute_likelihood_x, brute_posterior)\n",
    "plt.xlabel('Mobility')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xticks(unique_mobilities[::4])\n",
    "\n",
    "# plt.savefig('figs_final/1d_MCMC_results/brute-posterior-plot-sample'+str(sample_id)+'.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ESS values of MCMC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ESS values (first pass):\n",
    "\n",
    "def check_ESS(sample_id):\n",
    "    MCMC_file = MCMC_results_folder+\"/AMCMC_sample_ind_\"+str(sample_id)+\".pickle\"\n",
    "    data_file = open(MCMC_file, \"rb\")\n",
    "    sol = pickle.load(data_file)\n",
    "    data_file.close()\n",
    "    \n",
    "    n = end-nburn\n",
    "    auto_corr = compute_group_auto_corr(sol['chain'][nburn:end], 200)\n",
    "    ESS, went_to_zero = compute_effective_sample_size(n,auto_corr[:,0])\n",
    "    \n",
    "    print('*')\n",
    "    \n",
    "    return [ESS, went_to_zero]\n",
    "\n",
    "total = 1000\n",
    "result = []\n",
    "for i in range(0,total):\n",
    "    result.append(check_ESS(i))\n",
    "    \n",
    "# total = 1000\n",
    "# num_workers = mp.cpu_count()\n",
    "# print(num_workers)\n",
    "# start_time = time.perf_counter()\n",
    "# with Pool(num_workers) as pool:\n",
    "#     result = pool.map(check_ESS, range(0,total))\n",
    "# finish_time = time.perf_counter()\n",
    "# print(\"Program finished in {} seconds - using multiprocessing\".format(finish_time-start_time))\n",
    "# print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ESS results\n",
    "user_input = input(\"Do you want to save the data? Type 'yes' or 'no': \")\n",
    "if user_input.lower() == 'yes': #So that I don't overwrite the data with zeros\n",
    "    print('Saving...')\n",
    "    with open(MCMC_results_folder+'/ESS.pickle', 'wb') as handle:\n",
    "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print('DID NOT SAVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESS results\n",
    "data_file = open(MCMC_results_folder+'/ESS.pickle', \"rb\") # Combined batch of MCMC\n",
    "result = pickle.load(data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update ESS results for samples that didn't have auto-correlation go to zero on the first pass:\n",
    "\n",
    "updated_result = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    if result[i][1] == True:\n",
    "        updated_result.append(result[i])\n",
    "    else:\n",
    "        MCMC_file = MCMC_results_folder + \"/AMCMC_sample_ind_\"+str(i)+\".pickle\"\n",
    "        data_file = open(MCMC_file, \"rb\")\n",
    "        sol = pickle.load(data_file)\n",
    "        data_file.close()\n",
    "        \n",
    "        time.sleep(1)\n",
    "        n = end-nburn\n",
    "        auto_corr = compute_group_auto_corr(sol['chain'][nburn:end], 3000)\n",
    "        ESS, went_to_zero = compute_effective_sample_size(n,auto_corr[:,0])\n",
    "        updated_result.append([ESS, went_to_zero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ESS results\n",
    "user_input = input(\"Do you want to save the data? Type 'yes' or 'no': \")\n",
    "if user_input.lower() == 'yes': #So that I don't overwrite the data with zeros\n",
    "    print('Saving...')\n",
    "    with open(MCMC_results_folder+'/refined_ESS.pickle', 'wb') as handle:\n",
    "        pickle.dump(updated_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print('DID NOT SAVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESS results\n",
    "data_file = open(MCMC_results_folder+'/refined_ESS.pickle', \"rb\") # Combined batch of MCMC\n",
    "updated_result = pickle.load(data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation based calibration\n",
    "Plotting reproduces Fig. 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nburn = 25000\n",
    "end = 75000\n",
    "L = 50 #trunctation point\n",
    "\n",
    "#--------------Pull results from folders---------------\n",
    "\n",
    "# # First batch of MCMC\n",
    "# MCMC_results_folder = \"1D_calibration_results_071123/MCMC_results_071123\"\n",
    "# brute_force_posterior_folder = \"1D_calibration_results_071123/Brute_force_posterior_estimation_071123\"\n",
    "\n",
    "# # Second batch of MCMC\n",
    "ranks = []\n",
    "ESS_too_small = []\n",
    "\n",
    "for sample_id in tqdm(range(1000)):\n",
    "    \n",
    "#     print('-------------------------')\n",
    "#     print('-------Sample '+str(sample_id)+'-------')\n",
    "#     print('-------------------------')\n",
    "    \n",
    "    #---------- Check MCMC results -----------\n",
    "    \n",
    "    MCMC_file = MCMC_results_folder+\"/AMCMC_sample_ind_\"+str(sample_id)+\".pickle\"\n",
    "    data_file = open(MCMC_file, \"rb\")\n",
    "    sol = pickle.load(data_file)\n",
    "    data_file.close()\n",
    "    \n",
    "    # Thin and truncate the chain based on effective sample size and chosen L\n",
    "    chain_thin_trunc = sol['chain'][nburn:end:int((end-nburn)/updated_result[sample_id][0])]\n",
    "    print(int((end-nburn)/updated_result[sample_id][0]))\n",
    "    print(chain_thin_trunc.shape)\n",
    "    chain_thin_trunc = chain_thin_trunc[:L]\n",
    "    \n",
    "    mob_value = mobilities[sample_id]\n",
    "#     upper_percentile = np.percentile(chain_thin_trunc, 97.5)\n",
    "#     lower_percentile = np.percentile(chain_thin_trunc, 2.5)\n",
    "#     upper_percentile_50 = np.percentile(chain_thin_trunc, 75)\n",
    "#     lower_percentile_50 = np.percentile(chain_thin_trunc, 25)\n",
    "    \n",
    "    if chain_thin_trunc.shape[0] < L:\n",
    "        quantile = np.searchsorted(np.sort(chain_thin_trunc.squeeze()),mob_value)/(chain_thin_trunc.shape[0])\n",
    "        closest_rank = round(quantile*(L))\n",
    "        ESS_too_small.append(sample_id)\n",
    "        ranks.append(closest_rank)\n",
    "    else:\n",
    "        ranks.append(np.searchsorted(np.sort(chain_thin_trunc.squeeze()),mob_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rank histogram:\n",
    "plt.figure(dpi = 200, figsize = (3,3))\n",
    "bins = np.arange(0,L+2)\n",
    "bins = np.arange(0,L+2,1)\n",
    "bins[-1] = L+1\n",
    "counts, bins = np.histogram(ranks, bins=bins, range=None, density=None, weights=None)\n",
    "plt.stairs(counts, bins-0.5, fill = True)\n",
    "plt.xlabel('Mobility rank')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Chi-squared test:\n",
    "vals = ranks \n",
    "n_bins = 51\n",
    "counts, bins = np.histogram(vals, bins=n_bins, range=None, density=None, weights=None)\n",
    "print('p-value:', scipy.stats.chisquare(counts).pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in training data\n",
    "# data_file = open('./Data/Training Data/One-parameter case/Calibration method 1/new_I_data_One-Pop-Disc.pickle', \"rb\")\n",
    "# data = pickle.load(data_file)\n",
    "# data_file.close()\n",
    "\n",
    "# #Load in training data mobility and jumping probability values:\n",
    "# parameter_matrix = pd.read_csv('./Data/Training Data/One-parameter case/Calibration method 1/variable_parameter_values_One-Pop-Disc.csv', index_col=0).to_numpy()\n",
    "# mobilities = parameter_matrix[:,0]\n",
    "# jumping_probs = parameter_matrix[:,1]\n",
    "# random_seeds = parameter_matrix[:,2]\n",
    "\n",
    "# #Reshape to sort by mobility and jumping prob:\n",
    "# #based on assumption that data is given in shape (num_of_sims, num_of_time_steps, num_of_subpops)\n",
    "# #and that within the list of simulations, the mobility is the \"outer\" index (changes more slowly)\n",
    "# #and the jumping probability is the \"inner\" index (changes more quickly)\n",
    "\n",
    "# unique_mobilities = np.unique(mobilities) \n",
    "# unique_jumping_probs = np.unique(jumping_probs)\n",
    "\n",
    "# num_of_mobilities = unique_mobilities.shape[0]\n",
    "# num_of_jumping_probs = unique_jumping_probs.shape[0]\n",
    "\n",
    "# num_of_random_seeds_per_param_set = round(data.shape[0]/(num_of_mobilities*num_of_jumping_probs)) #assuming equal number of random seeds run for each param set\n",
    "# num_of_time_steps = data.shape[1]\n",
    "# # num_of_subpops = 2 #assumes 2 sub-populations\n",
    "\n",
    "# unique_mobilities = np.unique(mobilities) \n",
    "# num_of_mobilities = unique_mobilities.shape[0]\n",
    "\n",
    "# data_sorted = np.zeros((num_of_mobilities, num_of_random_seeds_per_param_set, num_of_time_steps))\n",
    "\n",
    "# data_raveled = np.ravel(data, order = 'C')\n",
    "# data = np.reshape(data_raveled, data_sorted.shape)\n",
    "# data = np.swapaxes(data, 0, 2)\n",
    "\n",
    "\n",
    "data_file = open('./Data/Test Data/One-parameter case/new_I_data_One-Pop-NEW-COMBINED-TEST.pickle', \"rb\") # Combined batch of MCMC\n",
    "data = pickle.load(data_file)\n",
    "data_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose sample to run posterior predictive check on:\n",
    "# sample_id = 9\n",
    "# L = 500 #number of posterior samples to truncate down to\n",
    "# nburn = 25000\n",
    "# end = 75000\n",
    "RUN_NAME = 'Posterior_predictive_check_runs'\n",
    "DIRECTORY = './Data/Posterior_predictive_check_runs'\n",
    "os.makedirs(DIRECTORY, exist_ok = True)\n",
    "\n",
    "def ppc(sample_id, L):\n",
    "    #Load data from this sample:\n",
    "    sample_data = data[sample_id, :]\n",
    "\n",
    "    #Load MCMC runs from this sample:\n",
    "    MCMC_file = MCMC_results_folder+\"/AMCMC_sample_ind_\"+str(sample_id)+\".pickle\"\n",
    "    data_file = open(MCMC_file, \"rb\")\n",
    "    sol = pickle.load(data_file)\n",
    "    data_file.close()\n",
    "\n",
    "    #Thin and truncate chain based on chosen L and ESS:\n",
    "    chain_thin_trunc = sol['chain'][nburn:end:int((end-nburn)/updated_result[sample_id][0])]\n",
    "    chain_thin_trunc = chain_thin_trunc[:L]\n",
    "\n",
    "    #Run simulations with the posterior samples:\n",
    "    #============================================================================================================\n",
    "    #============================================= Set parameters ===============================================\n",
    "    #============================================================================================================\n",
    "\n",
    "    #parameters\n",
    "    m = 1 #number of populations\n",
    "    centers = np.array([[0.05,0.05]]) #theta, phi or x, y\n",
    "    spread = np.array([0.1,0.1]) #standard deviation of normal distribution\n",
    "    pop = np.array([100,100]) #population\n",
    "    A_1 = 0.0002 #theta or x mobility (azimuth mobility)\n",
    "    A_2 = 0.0002 #phi or y mobility (inclination mobility)\n",
    "    R = 1 #radius\n",
    "    d_IU = 0.005\n",
    "    E_0 = np.array([0,0]) #fraction of initially exposed\n",
    "    I_0 = np.array([0.01,0]) #fraction of initially infected\n",
    "    S_0 = np.array([0.99,1]) #fraction of initially susceptible\n",
    "    T_E = 11.6 #time from exposure to infectious\n",
    "    T_E_stdev = 1.9 #standard deviation of exposure time\n",
    "    T_I = 18.49 #incubation time\n",
    "    T_I_stdev = 3.71 #standard deviation of infection time\n",
    "    del_t = 0.1 #time step\n",
    "    verlet_iter = 300 #number of steps between updating verlet list #CHANGE THIS LATER\n",
    "    T = 300\n",
    "    rand_seed = 1\n",
    "    g = None\n",
    "    al = None\n",
    "    jumping_times = np.ones(int(T/del_t)+1)\n",
    "    jump_prob = 0.1\n",
    "    spherical = None\n",
    "    random_seed = 1\n",
    "    dist = 'Gamma'\n",
    "\n",
    "    #============================================================================================================\n",
    "    #=======================Set up population information files as C++ code inputs=========================\n",
    "    #============================================================================================================\n",
    "\n",
    "    num_of_sims = L\n",
    "    num_of_runs = 5 #split up runs to prevent losing all data at once if run fails, and to avoid slowdowns from high C++ code memory usage\n",
    "    num_of_sims_per_run = int(num_of_sims/num_of_runs)\n",
    "\n",
    "    if os.path.exists(DIRECTORY) == False:\n",
    "        os.makedirs(DIRECTORY)\n",
    "        os.makedirs(DIRECTORY+'/Pop info stored in parts')\n",
    "        os.makedirs(DIRECTORY+'/Data stored in parts')\n",
    "\n",
    "    np.random.seed(2)\n",
    "\n",
    "\n",
    "    #Set mobility values based on parameters pulled from posterior:\n",
    "    mobilities = chain_thin_trunc.squeeze()\n",
    "    jump_probs = 0*np.random.rand(num_of_sims) #jumping probabilities are all 0\n",
    "\n",
    "    pop_info_array = np.zeros((num_of_sims,14)) \n",
    "\n",
    "    #Set population sizes:\n",
    "    pop_info_array[:,0] = 100 #set all sub-populations to 100\n",
    "\n",
    "    #Set status fractions (one-population scenario):\n",
    "    pop_info_array[:,1] = 0.99 #set S fraction to 0.99\n",
    "    pop_info_array[:,2] = 0 #set E to 0\n",
    "    pop_info_array[:,3] = 0.01 #set I fraction to 0.01\n",
    "\n",
    "    #Set domains (one-population scenario):\n",
    "    pop_info_array[:,5] = 0 #set negative x-lim to 0 for first sub-pop\n",
    "    pop_info_array[:,6] = 0.1 #set negative x-lim to 0 for first sub-pop\n",
    "    pop_info_array[:,7] = 0 #set negative y-lim to 0 for all sub-pops\n",
    "    pop_info_array[:,8] = 0.1 #set negative y-lim to 0 for all sub-pops\n",
    "\n",
    "    #Set sub-simulation IDs:\n",
    "    pop_info_array[:,9] = np.arange(num_of_sims)\n",
    "\n",
    "    #Set seeds:\n",
    "    pop_info_array[:,10] = np.arange(num_of_sims, num_of_sims+num_of_sims)+80000000 #use different random seeds for new test data\n",
    "\n",
    "    #Set m values (m = num of subpopulations for a given subsim):\n",
    "    pop_info_array[:,11] = int(1)\n",
    "\n",
    "    #Set mobilities:\n",
    "    pop_info_array[:,12] = mobilities\n",
    "\n",
    "    #Set jumping probabilities\n",
    "    pop_info_array[:,13] = jump_probs\n",
    "\n",
    "    # user_input = input(\"Do you want to save the data? Type 'yes' or 'no': \")\n",
    "    # if user_input.lower() == 'yes': #So that I don't overwrite the data with zeros\n",
    "    print('Saving pop info...')\n",
    "    df = pd.DataFrame(pop_info_array, columns = ['Total population','S','E','I','DR','x_neg_lim','x_pos_lim','y_neg_lim', 'y_pos_lim', 'Sub-simulation', 'Seed', 'm', 'Mobility', 'Jumping probabilities'], dtype=object)\n",
    "    df.to_csv(DIRECTORY+'/pop-info-FULL.csv', index = False)\n",
    "\n",
    "    for i in range(num_of_runs):\n",
    "        to_write = pop_info_array[num_of_sims_per_run*i:num_of_sims_per_run*i+num_of_sims_per_run,:]\n",
    "        to_write[:, 9] = np.arange(num_of_sims_per_run) \n",
    "        df = pd.DataFrame(to_write, columns = ['Total population','S','E','I','DR','x_neg_lim','x_pos_lim','y_neg_lim', 'y_pos_lim', 'Sub-simulation', 'Seed', 'm', 'Mobility', 'Jumping probabilities'], dtype=object)\n",
    "        df.to_csv(DIRECTORY+'/Pop info stored in parts/pop-info-part'+str(i)+'.csv', index = False)\n",
    "    print('Done saving pop info.')\n",
    "\n",
    "    # else:\n",
    "    #     print('DID NOT SAVE -- CHANGE VARIABLE allow_save TO 1 TO ALLOW SAVING')\n",
    "\n",
    "    # allow_s0ve=0\n",
    "\n",
    "    #============================================================================================================\n",
    "    #================================== Run simulations and save data ===========================================\n",
    "    #============================================================================================================\n",
    "    # Run simulations and save data:\n",
    "    simulate_epidemic_1d(m, centers, spread, pop, A_1, A_2, R, d_IU, E_0, I_0, S_0, T_E, T_E_stdev, \n",
    "                      T_I, T_I_stdev, del_t, verlet_iter, T, rand_seed, g, al, jumping_times, jump_prob, \n",
    "                      spherical, random_seed, dist, num_of_sims, num_of_runs, RUN_NAME, DIRECTORY)\n",
    "    \n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_t = 0.1 #time step\n",
    "T = 300\n",
    "time_vec = np.linspace(0,T,int(T/del_t)+1)\n",
    "\n",
    "test_seeds = np.random.permutation(np.arange(0,1000))\n",
    "within_95_conf = []\n",
    "for i in range(0,100):\n",
    "    print('i = ', i)\n",
    "    sample_id = test_seeds[i]\n",
    "    L = 500\n",
    "    sample_data = ppc(sample_id, L)\n",
    "\n",
    "    #Load posterior predictive data:\n",
    "    data_file = open(DIRECTORY+'/new_I_data_'+RUN_NAME+'.pickle', \"rb\") # Combined batch of MCMC\n",
    "    predictive_data = pickle.load(data_file)\n",
    "    data_file.close()\n",
    "\n",
    "    def rolling_window_accum(data, steps_to_acc):\n",
    "        I_ABM_ALL=np.zeros_like(data)\n",
    "        for i in range(steps_to_acc):\n",
    "            if i==0:\n",
    "                I_ABM_ALL+=data*1 # Hard copy\n",
    "            else:\n",
    "                I_ABM_ALL[:,i:]+=data[:,:-i]\n",
    "        return I_ABM_ALL\n",
    "\n",
    "    def rolling_window_accum_1d(data, steps_to_acc):\n",
    "        I_ABM_ALL=np.zeros_like(data)\n",
    "        for i in range(steps_to_acc):\n",
    "            if i==0:\n",
    "                I_ABM_ALL+=data*1 # Hard copy\n",
    "            else:\n",
    "                I_ABM_ALL[i:]+=data[:-i]\n",
    "        return I_ABM_ALL\n",
    "\n",
    "    acc_data = rolling_window_accum(predictive_data, 3001)\n",
    "\n",
    "    #Plot 95% CI\n",
    "    lower_conf_bound = np.percentile(acc_data, 2.5, axis = 0)\n",
    "    upper_conf_bound = np.percentile(acc_data, 97.5, axis = 0)\n",
    "    median = np.percentile(acc_data, 50, axis = 0)\n",
    "\n",
    "    plt.figure(dpi = 500, figsize = (4, 3))\n",
    "    plt.plot(time_vec, rolling_window_accum_1d(sample_data, 3001), label = 'Observed data')\n",
    "    plt.plot(time_vec, upper_conf_bound, label = 'Upper 95% CI bound')\n",
    "    plt.plot(time_vec, lower_conf_bound, label = 'Lower 95% CI bound')\n",
    "    # plt.plot(time_vec, median, label = 'Median')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Total infections')\n",
    "    \n",
    "    # plt.savefig('figs_final/MCMC_ppc/CI_plot_sample_id_'+str(sample_id)+'.png',bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    sample_data_acc = rolling_window_accum_1d(sample_data, 3001)\n",
    "\n",
    "    print(\"Observed data is within CI's:\", np.min(np.array((lower_conf_bound<=sample_data_acc) & (sample_data_acc<=upper_conf_bound))))\n",
    "    within_95_conf.append(np.min(np.array((lower_conf_bound<=sample_data_acc) & (sample_data_acc<=upper_conf_bound))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABM_env",
   "language": "python",
   "name": "abm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15 | packaged by conda-forge | (main, Sep 20 2024, 16:31:41) [Clang 17.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
